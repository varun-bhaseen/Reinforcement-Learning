{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\bhase\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\bhase\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\bhase\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\bhase\\anaconda3\\lib\\site-packages (from gym) (1.19.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\bhase\\anaconda3\\lib\\site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: future in c:\\users\\bhase\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have imported the gym package and created an environment called CartPole.\n",
    "\n",
    "The observation of this environment is four floating-point numbers containing information about the x coordinate of the stick's center of mass, its speed, its angle to the platform, and its angular speed\n",
    "\n",
    "The reward in this environment is 1, and it is given on every time step. The episode continues until the stick falls, so to get a more accumulated reward, we need to balance the platform in a way to avoid the stick falling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03456031, -0.01822479, -0.04376858, -0.01860489])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reset the environment and obtained the first observation\n",
    "\n",
    "The action_space field is of the Discrete type, so our actions will be just 0 or 1, where 0 means pushing the platform to the left and 1 means to the right. The observation space is of Box(4,), which means a vector of size 4 with values inside the [âˆ’inf, inf] interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0349248 , -0.21269262, -0.04414068,  0.25995362]), 1.0, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pushed our platform to the left by executing the action 0 and got the tuple of four elements:\n",
    "*    A new observation, which is a new vector of four numbers  \n",
    "*    A reward of 1.0  \n",
    "*    The done flag with value False, which means that the episode is not over yet and we are more or less okay  \n",
    "*    Extra information about the environment, which is an empty dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.17177486e+00, -4.02094942e+37,  1.23289436e-01,  3.11900481e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8129265e-01, -1.8738752e+38, -2.9526111e-01,  7.3845509e+36],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returned a random sample from the underlying space, which in the case of our Discrete action space means a random number of 0 or 1, and for the observation space means a random vector of four numbers. The random sample of the observation space may not look useful, and this is true, but the sample from the action space could be used when we are not sure how to perform an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 20 steps, total reward 20.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOn average, our random agent takes 12 to 15 steps before the pole falls and the \\nepisode ends. Most of the environments in Gym have a \"reward boundary,\" which is \\nthe average reward that the agent should gain during 100 consecutive episodes to \\n\"solve\" the environment. For CartPole, this boundary is 195, which means that, on \\naverage, the agent must hold the stick for 195 time steps or longer.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    \"\"\"\n",
    "    Here, we created the environment and initialized the counter of steps and \n",
    "    the reward accumulator. On the last line, we reset the environment to \n",
    "    obtain the first observation \n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        \"\"\"\n",
    "        In this loop, we sampled a random action, then asked the environment to \n",
    "        execute it and return to us the next observation (obs), the reward, and \n",
    "        the done flag. If the episode is over, we stop the loop and show how \n",
    "        many steps we have taken and how much reward has been accumulated.\n",
    "        \"\"\"\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (\n",
    "        total_steps, total_reward))\n",
    "    \n",
    "\"\"\"\n",
    "On average, our random agent takes 12 to 15 steps before the pole falls and the \n",
    "episode ends. Most of the environments in Gym have a \"reward boundary,\" which is \n",
    "the average reward that the agent should gain during 100 consecutive episodes to \n",
    "\"solve\" the environment. For CartPole, this boundary is 195, which means that, on \n",
    "average, the agent must hold the stick for 195 time steps or longer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
